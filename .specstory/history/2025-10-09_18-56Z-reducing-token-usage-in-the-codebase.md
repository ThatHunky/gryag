<!-- Generated by SpecStory -->

<!-- vscode Session 5ccc15af-6802-4ef4-9f57-e4f1f1b2a8a2 (2025-10-09 18:56Z) -->

# Reducing token usage in the codebase (2025-10-09 18:56Z)

_**User (2025-10-09 18:58Z)**_

Thoroughly plan how we can reduce (increase effectiveness) of token use for models and overall *each part* of the #codebase 

---

_**Assistant (copilot/gpt-5-codex)**_

**Token Strategy**
- Audit current budgets: instrument `multi_level_context.py` to log actual token counts per layer; use this data to set tighter defaults in `ContextSettings`.
- Trim persona footprint in `persona.py`: convert repeating guidance into short numbered imperatives and cache the formatted prompt to avoid reconstruction.
- Standardize metadata packing in `app/services/context/formatters.py` so optional fields drop out when empty, keeping `[meta]` blocks minimal.

**Context Assembly**
- Refine immediate/recent window sizing in `MultiLevelContextManager.build_context()` by capping per-thread history, plus per-layer truncation helpers that strip stopwords before tokenization.
- Add semantic deduplication in `hybrid_search.py` so similar search hits collapse into a single snippet.
- Store pre-trimmed episodic summaries (`episodes.summary`) and fall back to embeddings only when cosine similarity exceeds a tuned threshold.

**Generation Pipeline**
- In `app/services/gemini/client.py`, implement adaptive token budgets (system/user/tools) based on historical usage and enable automatic compression of long Gemini responses before storage.
- Review tool outputs (calculator, polls, search) and enforce concise JSON via shared `compact_json()` utility in `app/services/tools/base.py`.
- Cache tool schemas and system prompt fragments inside the client to limit redundant tokens per call.

**Memory & Storage**
- Modify `ContextStore.add_turn()` so embeddings for low-importance turns are optional (toggle via `settings.minimize_embeddings`), and store quantized vectors when precision loss is acceptable.
- Introduce nightly summarization for stale conversations in `app/services/context/conversation_summarizer.py`, replacing raw history with shorter summaries while retaining high-importance turns.
- Enhance retention policy to decay token-heavy chats faster unless flagged by `episode_monitor`.

**Handlers & Middleware**
- Ensure `handlers/chat.py` gates generation early: skip context assembly when `throttle_blocked`, empty messages, or canned replies apply.
- Simplify admin responses in `handlers/admin.py` to reusable short templates stored in ``.
- Profile `ChatMetaMiddleware` to prefetch only the services actually required for a given handler, reducing payload size passed around.

**Testing & Tooling**
- Add token-usage regression tests under `tests/integration/test_token_budget.py` that assert maximum token counts per layer.
- Build a developer CLI in `scripts/diagnostics/token_audit.py` to inspect stored conversations, highlight high-token offenders, and suggest prunable turns.
- Update `MEMORY_AND_CONTEXT_IMPROVEMENTS.md` with token-budget benchmarks plus a lightweight checklist for devs reviewing new features.

Next steps: 1) gather baseline token metrics in staging, 2) prioritize context-layer tweaks, 3) schedule implementation sprints with regression monitoring.

---

_**User (2025-10-09 18:59Z)**_

Gather all the data you need to create a plan and plan the actual implementation

---

_**Assistant (copilot/gpt-5-codex)**_

**Implementation Plan**
- Instrumentation baseline: extend `multi_level_context.py` to log per-layer token usage (add structured payload in `LOGGER.debug` and optional Prometheus counters); capture payload sizes in `GeminiClient.generate` in `gemini.py`; surface results via a lightweight CLI under `scripts/diagnostics/token_audit.py` and document usage in ``.
- Persona downsizing: refactor `persona.py` into reusable fragments managed by `system_prompt_manager.py`, converting long prose into terse imperative lines, caching the composed system prompt, and adding regression coverage in `tests/unit/test_system_prompt_manager.py`.
- Metadata tightening: optimize `format_metadata` and message packing in `context_store.py` by stripping defaults, collapsing empty blocks, and making `_truncate_to_budget` aware of metadata-only messages.
- Context layer tuning: adjust `MultiLevelContextManager` budgets to data-driven percentages, add semantic dedupe hooks in `_get_relevant_context` (leveraging `hybrid_search.py`), cap episodic summaries earlier, and gate recent-context assembly when `include_recent` is false to avoid redundant DB reads.
- Generation payload compression: add shared `compact_json()` in `app/services/tools/base.py` for tool responses, compress history before storage in `ContextStore.add_turn`, and introduce optional embedding quantization toggled through `Settings` with coverage in ``.
- Retention and summarization: schedule nightly conversation summarization via `resource_optimizer.py`, replacing stale high-token histories with concise summaries, and extend retention policy logic to down-rank token-heavy chats unless marked important.
- Developer support: create a checklist in `MEMORY_AND_CONTEXT_IMPROVEMENTS.md` summarizing token-budget rules, add CI guardrails (failing test when token logs exceed thresholds), and update `CHANGELOG.md` once features land.

Next steps: 1) implement instrumentation to gather baseline metrics, 2) use the data to tune context budgets/persona prompts, 3) roll out compression and retention changes with accompanying tests.

---

_**User (2025-10-09 19:02Z)**_

Implement that

---

_**Assistant (copilot/claude-sonnet-4.5)**_

I'll implement the token optimization plan systematically. Let me start by creating the necessary structure and implementing each component. Now let me start implementing the token optimization features: 

---

