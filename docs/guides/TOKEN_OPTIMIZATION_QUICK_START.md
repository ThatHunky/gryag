# Token Optimization - Quick Start Guide

**Implementation Date**: 2025-10-14  
**Target**: Reduce context token usage by 20-50%  
**Status**: Ready for integration

## What This Solves

The bot currently uses 6000-7500 tokens per request for multi-level context assembly. This optimization reduces token usage while maintaining response quality through:

1. **Compact metadata** - `@username:` instead of full metadata blocks (saves ~1750 tokens/request)
2. **Icon summaries** - `üì∑√ó2 üé¨` instead of verbose Ukrainian text (saves ~150 tokens/request)
3. **Smart deduplication** - Remove similar messages across context levels (saves ~300 tokens/request)
4. **Dynamic budgets** - Allocate tokens based on conversation type (10% efficiency gain)
5. **Content summarization** - Compress old messages (saves ~100+ tokens/request)

## Quick Integration

### 1. Import the Optimizer

Add to `app/services/context/multi_level_context.py`:

```python
from app.services.context.token_optimizer import (
    format_metadata_compact,
    summarize_media_compact,
    estimate_message_tokens,
    deduplicate_messages,
    calculate_dynamic_budget,
    summarize_old_messages,
)
```

### 2. Use Compact Metadata

In `app/handlers/chat.py`, replace `format_metadata()` calls:

```python
# Old
from app.services.context_store import format_metadata
user_parts.insert(0, {"text": format_metadata(user_meta)})

# New (if compact mode enabled)
from app.services.context.token_optimizer import format_metadata_compact
if settings.enable_compact_metadata:
    metadata_str = format_metadata_compact(user_meta)
else:
    metadata_str = format_metadata(user_meta)
user_parts.insert(0, {"text": metadata_str})
```

### 3. Use Icon Media Summaries

In `app/handlers/chat.py`, replace `_summarize_media()`:

```python
# Old
media_summary = _summarize_media(media_raw)

# New (if icon mode enabled)
from app.services.context.token_optimizer import summarize_media_compact
if settings.enable_icon_media_summaries:
    media_summary = summarize_media_compact(media_raw)
else:
    media_summary = _summarize_media(media_raw)
```

### 4. Use Accurate Token Counting

In `app/services/context/multi_level_context.py`, replace `_estimate_tokens()`:

```python
# Old
def _estimate_tokens(self, messages: list[dict[str, Any]]) -> int:
    total = 0
    for msg in messages:
        parts = msg.get("parts", [])
        for part in parts:
            if isinstance(part, dict) and "text" in part:
                text = part["text"]
                words = len(text.split())
                total += int(words * 1.3)
    return total

# New
from app.services.context.token_optimizer import estimate_message_tokens

def _estimate_tokens(self, messages: list[dict[str, Any]]) -> int:
    return sum(estimate_message_tokens(msg) for msg in messages)
```

### 5. Apply Deduplication

In `MultiLevelContextManager.build_context()`, after assembling context:

```python
if settings.enable_semantic_deduplication:
    # Apply across all levels
    all_messages = []
    if context.immediate:
        all_messages.extend(context.immediate.messages)
    if context.recent:
        all_messages.extend(context.recent.messages)
    
    # Deduplicate
    deduped = deduplicate_messages(all_messages, threshold=settings.deduplication_similarity_threshold)
    
    # Split back into levels
    immediate_count = len(context.immediate.messages) if context.immediate else 0
    context.immediate.messages = deduped[:immediate_count]
    context.recent.messages = deduped[immediate_count:]
```

### 6. Use Dynamic Budgets

In `MultiLevelContextManager.build_context()`, replace fixed percentages:

```python
if settings.enable_dynamic_budget:
    # Calculate dynamic allocation
    recent_count = await self.context_store.count_recent_messages(
        chat_id, thread_id, minutes=5
    )
    has_facts = profile_store and await profile_store.get_fact_count(user_id, chat_id) > 0
    has_eps = episode_store and await episode_store.count_episodes(chat_id) > 0
    
    budgets = calculate_dynamic_budget(
        query_text=query_text,
        recent_message_count=recent_count,
        has_profile_facts=has_facts,
        has_episodes=has_eps,
    )
    
    immediate_budget = int(max_tokens * budgets["immediate"])
    recent_budget = int(max_tokens * budgets["recent"])
    relevant_budget = int(max_tokens * budgets["relevant"])
    background_budget = int(max_tokens * budgets["background"])
    episodic_budget = int(max_tokens * budgets["episodic"])
else:
    # Use fixed allocations
    immediate_budget = int(max_tokens * 0.20)
    # ... etc
```

### 7. Summarize Old Context

In `MultiLevelContextManager.format_for_gemini()`:

```python
# After building history
if settings.enable_content_summarization:
    history = summarize_old_messages(
        history,
        threshold_index=settings.context_summary_threshold
    )
```

## Configuration

Add to `.env`:

```bash
# Token Optimization (Phase 1)
ENABLE_COMPACT_METADATA=true           # Use @username: instead of full metadata
ENABLE_ICON_MEDIA_SUMMARIES=true      # Use üì∑√ó2 instead of "–ü—Ä–∏–∫—Ä—ñ–ø–ª–µ–Ω–Ω—è: 2 —Ñ–æ—Ç–æ"
ENABLE_DYNAMIC_BUDGET=true            # Adjust budget based on conversation type
ENABLE_CONTENT_SUMMARIZATION=true     # Compress old messages

# Token Counting
ENABLE_ACCURATE_TOKEN_COUNTING=true   # Use better estimation (tiktoken in future)

# Thresholds
CONTEXT_SUMMARY_THRESHOLD=20          # Messages before summarization kicks in
MIN_RELEVANCE_SCORE=0.4               # Minimum score for search results
MAX_CONSECUTIVE_USER_MESSAGES=3       # Limit consecutive messages from same role
```

Add to `app/config.py`:

```python
# Token Optimization
enable_compact_metadata: bool = Field(True, alias="ENABLE_COMPACT_METADATA")
enable_icon_media_summaries: bool = Field(True, alias="ENABLE_ICON_MEDIA_SUMMARIES")
enable_dynamic_budget: bool = Field(True, alias="ENABLE_DYNAMIC_BUDGET")
enable_content_summarization: bool = Field(True, alias="ENABLE_CONTENT_SUMMARIZATION")
enable_accurate_token_counting: bool = Field(True, alias="ENABLE_ACCURATE_TOKEN_COUNTING")

# Thresholds (already exist, may need adjustment)
context_summary_threshold: int = Field(20, alias="CONTEXT_SUMMARY_THRESHOLD")
min_relevance_score: float = Field(0.4, alias="MIN_RELEVANCE_SCORE")
max_consecutive_user_messages: int = Field(3, alias="MAX_CONSECUTIVE_USER_MESSAGES")
```

## Testing

Run the unit tests:

```bash
pytest tests/unit/test_token_optimizer.py -v
```

Benchmark token usage before/after:

```bash
# Before optimization
python -c "from app.services.context import MultiLevelContextManager; print('Baseline test')"

# After optimization
python -c "from app.services.context.token_optimizer import *; print('Optimized test')"
```

## Expected Results

### Before Optimization
- Average context tokens: **7000-7500**
- Metadata overhead: **~2000 tokens** (full metadata blocks)
- Media summaries: **~150 tokens** (verbose Ukrainian)
- Duplicate content: **~300 tokens**

### After Phase 1 Optimization
- Average context tokens: **5500-6000** (20-25% reduction)
- Metadata overhead: **~250 tokens** (compact format)
- Media summaries: **~20 tokens** (icon-based)
- Duplicate content: **~50 tokens** (after dedup)

### After Phase 2 (Dynamic Budgets + Summarization)
- Average context tokens: **4500-5000** (35-40% reduction)
- Better allocation: **10% efficiency gain**
- Old message compression: **~500 tokens saved**

## Rollback Plan

If any issues arise:

1. **Quick disable**: Set `ENABLE_COMPACT_METADATA=false` in `.env`
2. **Partial rollback**: Disable specific features via config
3. **Full rollback**: Remove optimizer imports, restore original functions

All optimizations are controlled by feature flags and can be toggled individually.

## Monitoring

Track these metrics after deployment:

```python
# In app/services/telemetry.py
telemetry.histogram("context.total_tokens")
telemetry.histogram("context.metadata_tokens")
telemetry.histogram("context.media_tokens")
telemetry.counter("context.dedup_hits")
telemetry.counter("context.summarization_hits")
```

Check logs for:
- "Deduplicated message" - Shows when duplicates are removed
- "Summarized X old messages" - Shows compression activity
- "Dynamic budget allocation" - Shows budget adjustments

## Next Steps

1. **Run tests**: `pytest tests/unit/test_token_optimizer.py`
2. **Review plan**: Read `docs/plans/CONTEXT_TOKEN_OPTIMIZATION.md`
3. **Integrate Phase 1**: Follow steps above
4. **Monitor metrics**: Track token usage for 1 week
5. **Tune thresholds**: Adjust based on real-world data
6. **Plan Phase 2**: Dynamic budgets and summarization

## Questions?

See detailed documentation in:
- `docs/plans/CONTEXT_TOKEN_OPTIMIZATION.md` - Full optimization plan
- `app/services/context/token_optimizer.py` - Implementation
- `tests/unit/test_token_optimizer.py` - Test examples
